# architecture
arch: vit_base
enc_arch: MAEViTEncoder
dec_arch: MAEViTDecoder

# wandb
proj_name: mae2d
run_name: ${proj_name}_${arch}_${dataset}
wandb_id:
disable_wandb: false

# dataset
dataset: btcv
data_path: /standard/mlia/MLIA_Team14/SelfMedMAE
tr_listfile: /standard/mlia/MLIA_Team14/SelfMedMAE/train_list.txt
va_listfile: /standard/mlia/MLIA_Team14/SelfMedMAE/val_list.txt

# output
output_dir: /standard/mlia/MLIA_Team14/outputs/${run_name}
ckpt_dir: ${output_dir}/ckpts

# data preprocessing
mean_std_type: MED
crop_min: 0.2

# trainer
trainer_name: MAETrainer
lr: 1.5e-4
batch_size: 32
vis_batch_size: 4
start_epoch: 0
warmup_epochs: 40
epochs: 800
workers: 4
resume:

# model - ALL REQUIRED PARAMETERS
patchembed: PatchEmbed2D
pos_embed_type: sincos
mask_ratio: 0.75
input_size: 224
patch_size: 16
in_chans: 1
encoder_embed_dim: 768
encoder_depth: 12
encoder_num_heads: 12
decoder_embed_dim: 256
decoder_depth: 8
decoder_num_heads: 8
fourier_embed_dim: 24 # ← MISSING - REQUIRED!
fourier_temperature: 0.01 # ← MISSING - REQUIRED!

# optimizer
type: adamw
beta1: 0.9
beta2: 0.95
weight_decay: 0.05

# logging
vis_freq: 1000
save_freq: 10
print_freq: 10

# distributed processing
gpu: 0
dist_url:
world_size: 1
multiprocessing_distributed: false
dist_backend: nccl
distributed:
rank: 0
ngpus_per_node:

# randomness
seed:

# debugging
debug: false
